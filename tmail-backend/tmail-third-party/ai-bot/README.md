# AIBot third party

We provide a mailet extension for OpenAI-compatible APIs integration. The goal is to provide a mail bot at e.g. `gpt@example.com` that replies to users' questions.

## Requirements

Before getting started, make sure you have:
1. **An OpenAI-compatible API** with:
   - A valid API key
   - An API endpoint URL
2. **A suitable LLM model** for your use case

## Configuration Files

### API configuration

All settings are centralized in the `ai.properties` file, to be placed into the `/root/conf/` directory.

Sample `ai.properties` configuration:
```properties
apiKey=demo
botAddress=gpt@example.com
model=lucie
baseURL=https://chat.lucie.example.com
```

**Parameter explanation:**

* `apiKey`: The API key for accessing the LLM API service.
* `botAddress`: The email address used to send replies generated by the LLM.
* `baseURL`: The URL of an LLM API compatible with OpenAI, e.g. https://ai.linagora.com/api. If left blank, defaults to the official OpenAI API base URL.
* `model`: Identifier of the LLM that generates the replies. Defaults to `gpt-4o-mini`.

You can use the langchain4j demo API at http://langchain4j.dev/demo/openai/v1 with the `gpt-4o-mini` model and `demo` API key for testing purposes.

#### Sanity check

Verify your configuration with the following command:

```bash
curl <baseURL>/chat/completions \
  -H "Authorization: Bearer <apiKey>" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "<model>",
    "messages": [
      {
        "role": "system",
        "content": "You are a helpful assistant."
      },
      {
        "role": "user",
        "content": "Why is the sky blue?"
      }
    ]
  }'
```

### Mailet configuration

Modify the `mailetcontainer.xml` file by adding the following lines:

```xml
<processor state="local-delivery" enableJmx="true">
    <matcher name="aibot-allowed" match="org.apache.james.mailetcontainer.impl.matchers.And">
        <!-- Only answer to local users -->
        <matcher match="SenderIsLocal"/>
        <matcher match="com.linagora.tmail.mailet.RecipientsContain={your bot address here}"/>
    </matcher>

    ...
    <mailet match="All" class="com.linagora.tmail.mailets.TmailLocalDelivery">
        <consume>false</consume>
    </mailet>

    <!-- Put the AIBotMailet after LocalDelivery so the GPT reply would come after the asking question -->
    <mailet match="aibot-allowed" class="com.linagora.tmail.mailet.AIBotMailet"/>
    <mailet match="All" class="Null"/>
</processor>
```

## Starting the AIBot

###  Clean Install (In-Memory)

The following steps will start the AIBot extension in memory, without rate-limiting.

1. **Compile the extension**

    Build the extension JAR with Maven:

    ```bash
    mvn clean install -DskipTests --am --pl :tmail-ai-bot
    ```

2. **Plug the extension in your TMail application**

      - Copy the generated JAR `target/tmail-ai-bot-jar-with-dependencies.jar` in the `/root/libs/` directory of your TMail application.
      - Declare the plugin in the `extensions.properties` file
          ```bash
          guice.extension.module=com.linagora.tmail.mailet.conf.AIBotModule,com.linagora.tmail.jmap.aibot.AiBotMethodModule
          ```

3. **Run the server**

    Use the following Docker command to run AIBot in memory. Make sure that you have modified the configuration files in the `sample_conf/` directory before running:

    ```bash
    docker run \
      --mount type=bind,source="$PWD/sample_conf/jwt_publickey",target="/root/conf/jwt_publickey" \
      --mount type=bind,source="$PWD/sample_conf/jwt_privatekey",target="/root/conf/jwt_privatekey" \
      --mount type=bind,source="$PWD/sample_conf/mailetcontainer.xml",target="/root/conf/mailetcontainer.xml" \
      --mount type=bind,source="$PWD/sample_conf/ai.properties",target="/root/conf/ai.properties" \
      --mount type=bind,source="$PWD/sample_conf/extensions.properties",target="/root/conf/extensions.properties" \
      --volume "$PWD/target/tmail-ai-bot-jar-with-dependencies.jar:/root/libs/tmail-ai-bot-jar-with-dependencies.jar" \
      linagora/tmail-backend-memory
    ```
### Command with RAG System Integration
To enhance context understanding, AIBot can be extended with a RAG (Retrieval-Augmented Generation) system. This system handles all emails in the mailbox except those in the trash or spam folders.

To enable RAG, you need to mount a configuration file called listeners.xml, which contains the RAG listener:
```xml
<listeners>
    <listener>
        <class>com.linagora.tmail.aibot.RagListener</class>
        <users>btellier@linagora.com,ptranvan@linagora.com</users>
    </listener>
</listeners>
```
The RagListener is responsible for event processing.

You can specify a whitelist of users authorized to use the RAG feature in the <users> tag.

To enable this functionality, add the following mount to your Docker command:
```bash
      --mount type=bind,source="$PWD/sample_conf/listener.xml",target="/root/conf/listeners.xml" \
```
***the final docker command to run the server with RAG listener will be looking like this:***
```bash
    docker run \
      --mount type=bind,source="$PWD/sample_conf/jwt_publickey",target="/root/conf/jwt_publickey" \
      --mount type=bind,source="$PWD/sample_conf/jwt_privatekey",target="/root/conf/jwt_privatekey" \
      --mount type=bind,source="$PWD/sample_conf/mailetcontainer.xml",target="/root/conf/mailetcontainer.xml" \
      --mount type=bind,source="$PWD/sample_conf/ai.properties",target="/root/conf/ai.properties" \
      --mount type=bind,source="$PWD/sample_conf/extensions.properties",target="/root/conf/extensions.properties" \
      --mount type=bind,source="$PWD/sample_conf/listeners.xml",target="/root/conf/listeners.xml" \
      --volume "$PWD/target/tmail-ai-bot-jar-with-dependencies.jar:/root/libs/tmail-ai-bot-jar-with-dependencies.jar" \
      linagora/tmail-backend-memory
```

### With the demo server

You can test the AIBot extension with the demo server.

Sample configuration files are located in the `demo/tmail` directory. Modify these files as needed, and proceed with the [demo instructions](../../../demo/README.md). The default bot address is `gpt@tmail.com`.

## Considerations for deployment

> [!NOTE] Scope of this section
> 
> AIBot is still in development phase. It lacks the robustness, scalability and security hardening required for real-world deployment.
>
> This section provides general guidance for operators who still wish to experiment with deploying AIBot in a production environment. These recommendations are **not a substitute for comprehensive security practices** and should be treated as starting points for curious readers.

<details>
  <summary>Read more...</summary>

---
The AIBot extension must be defended against [cybersecurity attacks](https://genai.owasp.org/llm-top-10/) for production deployment. Recommended configuration steps are described below.

### Rate-limiting

This section provides more details on configuring rate limits for AIBot.

#### Threat model

Rate-limiting is critical to mitigate [Denial of Service attacks on LLMs](https://genai.owasp.org/llmrisk/llm102025-unbounded-consumption/). Indeed, a malicious user could send a high volume of emails to AIBot, forcing the extension to make API requests to the LLM service for each interaction. This could lead to:

- **Server overload** on the email or the LLM provider, potentially causing service degradation or complete unavailability.
- [**Denial of Wallet**](https://www.sciencedirect.com/science/article/pii/S221421262100079X) due to API credit overconsumption, which could cause financial exhaustion and service blocking by the LLM provider.

#### Configuration

Use the [`rate-limiter` mailet](https://github.com/apache/james-project/tree/master/server/mailet/rate-limiter) from Apache James by following its setup instructions.

We suggest to add at least two rate-limiting rules in `mailetcontainer.xml` such as below:

```xml
<processor state="local-delivery" enableJmx="true">
    <matcher name="aibot-allowed" match="org.apache.james.mailetcontainer.impl.matchers.And">
        <matcher match="com.linagora.tmail.mailet.RecipientsContain={your bot address here}"/>
        <matcher match="SenderIsLocal"/>
    </matcher>
    ...
    <mailet match="All" class="com.linagora.tmail.mailets.TmailLocalDelivery">
        <consume>false</consume>
    </mailet>

    <!-- Put the rate limit before AIBotMailet -->
    <mailet match="aibot-allowed" class="PerSenderRateLimit">
        <keyPrefix>AIBotPerSenderRateLimit</keyPrefix>
        <duration>1d</duration>
        <precision>1h</precision>
        <count>100</count>
        <size>100K</size>
        <exceededProcessor>tooMuchMails</exceededProcessor>
    </mailet>
    <mailet match="aibot-allowed" class="PerRecipientRateLimit">
        <keyPrefix>AIBotRecipientRateLimit</keyPrefix>
        <duration>1d</duration>
        <precision>1h</precision>
        <count>1000</count>
        <size>100K</size>
        <exceededProcessor>tooMuchMails</exceededProcessor>
    </mailet>
    <mailet match="aibot-allowed" class="com.linagora.tmail.mailet.AIBotMailet"/>
</processor>

<processor state="tooMuchMails" enableJmx="true">
    <mailet match="All" class="Bounce">
        <message>Rate limit exceeded!</message>
    </mailet>
</processor>
```

**Configuration explanation:**

1. **PerSenderRateLimit**

    This mailet bounds the mails _per sender_ to the bot address, which addresses Denial of Service attacks.

2. **PerRecipientRateLimit**

    This mailet bounds all the mails received by AIBot, regardless of the sender, preventing [Denial of Wallet attacks](https://genai.owasp.org/llmrisk/llm102025-unbounded-consumption/).

You must modify these values taking into account your threat model (e.g. number of email accounts used for DDoS) and LLM-specific factors such as budget, server performance and context size. Beware that the email headers add a constant factor to all mails, and the traffic volume of an email thread is quadratic with respect to its length. You may also configure [throttling](https://github.com/apache/james-project/tree/master/server/mailet/rate-limiter#throttling) if the bot answers don't need to be instantaneous.

### Controlling costs of LLM answers

LLMs can consume significant amounts of API credits, especially when generating long responses. If left unbounded, this can lead to Denial of Wallet attacks.

If you can configure the default chat parameters of your model, we recommend to set the following parameters according to your needs:

- maximum completion tokens 
- reasoning effort (if applicable)

</details>

## Troubleshooting

### No response received

1. [Verify your API configuration](#sanity-check)
2. Make sure the same bot address is used in the mailet configuration and in the properties file

### API quota issues

Demo APIs often have usage quotas. Ensure your requests are not being rate limited due to heavy usage or automated scripts.